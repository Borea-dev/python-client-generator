# generated by borea

# if you want to edit this file, add it to ignores in borea.config.json, glob syntax

# TODO: not implemented

from typing import Any, Dict, List, Optional, Union, TYPE_CHECKING
from ....models.models import *

if TYPE_CHECKING:
    from ...trieve_api import TrieveApi


class GenerateOffChunks:
    def __init__(self, parent: "TrieveApi"):
        self.parent = parent

    def generate_off_chunks(
        self,
        tr_dataset: str,
        chunk_ids: List[str],
        prev_messages: List[ChatMessageProxy],
        audio_input: Optional[str] = None,
        context_options: Optional[ContextOptions] = None,
        frequency_penalty: Optional[float] = None,
        highlight_results: Optional[bool] = None,
        image_config: Optional[ImageConfig] = None,
        image_urls: Optional[List[str]] = None,
        max_tokens: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        prompt: Optional[str] = None,
        stop_tokens: Optional[List[str]] = None,
        stream_response: Optional[bool] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> Any:
        """
        This endpoint exists as an alternative to the topic+message resource pattern where our Trieve handles chat memory. With this endpoint, the user is responsible for providing the context window and the prompt and the conversation is ephemeral.

        Args:
            tr_dataset: The dataset id or tracking_id to use for the request. We assume you intend to use an id if the value is a valid uuid.
            chunk_ids: The ids of the chunks to be retrieved and injected into the context window for RAG.
            prev_messages: The previous messages to be placed into the chat history. There must be at least one previous message.
            audio_input: Audio input to be used in the chat. This will be used to generate the audio tokens for the model. The default is None.
            context_options: Context options to use for the completion. If not specified, all options will default to false.
            frequency_penalty: Frequency penalty is a number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Default is 0.7.
            highlight_results: Set highlight_results to false for a slight latency improvement (1-10ms). If not specified, this defaults to true. This will add `<mark><b>` tags to the chunk_html of the chunks to highlight matching splits.
            image_config: Configuration for sending images to the llm
            image_urls: Image URLs to be used in the chat. These will be used to generate the image tokens for the model. The default is None.
            max_tokens: The maximum number of tokens to generate in the chat completion. Default is None.
            presence_penalty: Presence penalty is a number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Default is 0.7.
            prompt: Prompt will be used to tell the model what to generate in the next message in the chat. The default is 'Respond to the previous instruction and include the doc numbers that you used in square brackets at the end of the sentences that you used the docs for:'. You can also specify an empty string to leave the final message alone such that your user's final message can be used as the prompt. See docs.trieve.ai or contact us for more information.
            stop_tokens: Stop tokens are up to 4 sequences where the API will stop generating further tokens. Default is None.
            stream_response: Whether or not to stream the response. If this is set to true or not included, the response will be a stream. If this is set to false, the response will be a normal JSON response. Default is true.
            temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default is 0.5.
            user_id: User ID is the id of the user who is making the request. This is used to track user interactions with the RAG results.

        Returns:
            Response data
        """
        path = f"/api/chunk/generate"
        params = {}
        headers = {}
        if tr_dataset is not None:
            headers["TR-Dataset"] = tr_dataset
        json_data = {
            "audio_input": audio_input if audio_input is not None else None,
            "chunk_ids": chunk_ids if chunk_ids is not None else None,
            "context_options": context_options if context_options is not None else None,
            "frequency_penalty": (
                frequency_penalty if frequency_penalty is not None else None
            ),
            "highlight_results": (
                highlight_results if highlight_results is not None else None
            ),
            "image_config": image_config if image_config is not None else None,
            "image_urls": image_urls if image_urls is not None else None,
            "max_tokens": max_tokens if max_tokens is not None else None,
            "presence_penalty": (
                presence_penalty if presence_penalty is not None else None
            ),
            "prev_messages": prev_messages if prev_messages is not None else None,
            "prompt": prompt if prompt is not None else None,
            "stop_tokens": stop_tokens if stop_tokens is not None else None,
            "stream_response": stream_response if stream_response is not None else None,
            "temperature": temperature if temperature is not None else None,
            "user_id": user_id if user_id is not None else None,
        }
        json_data = {k: v for k, v in json_data.items() if v is not None}

        response = self.parent._make_request(
            method="POST",
            path=path,
            params=params,
            headers=headers,
            json_data=json_data,
        )
        return response.json()
